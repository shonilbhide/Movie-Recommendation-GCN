{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQo0BaoHwGIR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data into a pandas DataFrame\n",
        "columns = [\"user_id\", \"age\", \"gender\", \"occupation\", \"zip_code\"]\n",
        "users_df = pd.read_csv(\"./u.user\", sep=\"|\", names=columns, encoding=\"latin-1\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(users_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users_df.info()"
      ],
      "metadata": {
        "id": "JuRO8_MGwPL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_df.nunique()"
      ],
      "metadata": {
        "id": "eoBlLfZpwRme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "def convert_zip_code(zip_code):\n",
        "    try:\n",
        "        # Try to convert the zip code to integer\n",
        "        int(zip_code)\n",
        "        return zip_code\n",
        "    except ValueError:\n",
        "        # If conversion fails, return '0000'\n",
        "        return '00000'\n",
        "\n",
        "# Apply the conversion function to the chosen column\n",
        "users_df['zip_code'] = users_df['zip_code'].apply(convert_zip_code)\n",
        "# Apply Min-Max Scaling to the chosen column\n",
        "users_df['age'] = scaler.fit_transform(users_df[['age']])\n",
        "users_df['zip_code'] = scaler.fit_transform(users_df[['zip_code']])"
      ],
      "metadata": {
        "id": "8e7OTEcGwTmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply label encoding to the chosen column\n",
        "users_df['gender'] = label_encoder.fit_transform(users_df['gender'])"
      ],
      "metadata": {
        "id": "RmhoGPrZwaxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_df.describe()"
      ],
      "metadata": {
        "id": "NNIrk_Z8wdbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentences = [\n",
        "    ['engineer', 'software', 'developer'],\n",
        "    ['doctor', 'physician', 'surgeon'],\n",
        "    ['teacher', 'educator', 'professor'],\n",
        "    ['artist', 'painter', 'musician'],\n",
        "]\n",
        "\n",
        "# Extract the 'occupation' column\n",
        "occupations = users_df['occupation'].unique().tolist()\n",
        "\n",
        "# Add the example sentences to the occupations\n",
        "all_sentences = sentences + [occupations]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(all_sentences, vector_size=3, window=1, min_count=1, workers=4)\n",
        "\n",
        "# Access the word vectors\n",
        "word_vectors = model.wv\n",
        "\n",
        "# Function to encode the 'occupation' column using Word2Vec\n",
        "def encode_occupation(occupation):\n",
        "    return word_vectors[occupation]\n",
        "\n",
        "# Apply the encoding function to create a new column 'occupation_vector'\n",
        "users_df['occupation_vector'] = users_df['occupation'].apply(encode_occupation)"
      ],
      "metadata": {
        "id": "DeyFdc_PwiOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_df.head()"
      ],
      "metadata": {
        "id": "sM3gu-ThwkeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_dimensions = pd.DataFrame(users_df['occupation_vector'].to_list(), columns=[f'occupation_dim_{i+1}' for i in range(model.vector_size)])\n",
        "\n",
        "# Concatenate the new columns with the original DataFrame\n",
        "users_df = pd.concat([users_df, vector_dimensions], axis=1)\n",
        "\n",
        "# Drop the original 'occupation' and 'occupation_vector' columns\n",
        "users_df.drop(['occupation', 'occupation_vector'], axis=1, inplace=True)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(users_df)"
      ],
      "metadata": {
        "id": "q72iTjQwwlAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Extract relevant columns for similarity calculation (excluding 'user_id')\n",
        "columns_for_similarity = ['age', 'gender', 'zip_code', 'occupation_dim_1', 'occupation_dim_2', 'occupation_dim_3']\n",
        "data_for_similarity = users_df[columns_for_similarity]\n",
        "\n",
        "# Calculate pairwise Euclidean distances between rows\n",
        "distances = euclidean_distances(data_for_similarity)\n",
        "\n",
        "# Create a DataFrame from the distance matrix\n",
        "distance_df = pd.DataFrame(distances, index=users_df['user_id'], columns=users_df['user_id'])\n",
        "\n",
        "# Display the similarity scores\n",
        "print(distance_df)"
      ],
      "metadata": {
        "id": "HghFTKKlwnHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the distance threshold\n",
        "distance_threshold = 0.3\n",
        "\n",
        "# Find pairs of user_ids below the threshold\n",
        "similar_user_pairs = []\n",
        "\n",
        "for i in range(len(distance_df.index)):\n",
        "    for j in range(i + 1, len(distance_df.columns)):\n",
        "        if distance_df.iloc[i, j] < distance_threshold:\n",
        "            similar_user_pairs.append((distance_df.index[i], distance_df.columns[j]))\n",
        "\n",
        "# Display the pairs of user_ids\n",
        "print(\"Pairs of user_ids with distance below the threshold:\")\n",
        "print(similar_user_pairs)"
      ],
      "metadata": {
        "id": "38xFTHtKwv-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path = \"u.data\"\n",
        "\n",
        "# Define column names based on the provided format\n",
        "columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
        "\n",
        "# Read the u.data file into a DataFrame\n",
        "u_data_df = pd.read_csv(file_path, sep=\"\\t\", names=columns)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(u_data_df.head())"
      ],
      "metadata": {
        "id": "Q_2WmtwPw0T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming u_data_df is your DataFrame containing u.data\n",
        "columns_to_extract = [\"user_id\", \"item_id\", \"rating\"]\n",
        "\n",
        "# Extract the specified columns and convert to a NumPy array\n",
        "u_data_array = u_data_df[columns_to_extract].to_numpy()\n",
        "\n",
        "# Display the NumPy array\n",
        "print(u_data_array)"
      ],
      "metadata": {
        "id": "5HYtRB9dw3mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_user_pairs_array = np.array([(pair[0], pair[1], 1) for pair in similar_user_pairs])\n",
        "print(similar_user_pairs_array)"
      ],
      "metadata": {
        "id": "U_Nk3LHGw57h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "click_f = u_data_array"
      ],
      "metadata": {
        "id": "5bBX8MSzw8N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trust_f = similar_user_pairs_array"
      ],
      "metadata": {
        "id": "8D4zHC6Tw9qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GRDataset(Dataset):\n",
        "\tdef __init__(self, data, u_items_list, u_users_list, u_users_items_list, i_users_list):\n",
        "\t\tself.data = data\n",
        "\t\tself.u_items_list = u_items_list\n",
        "\t\tself.u_users_list = u_users_list\n",
        "\t\tself.u_users_items_list = u_users_items_list\n",
        "\t\tself.i_users_list = i_users_list\n",
        "\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tuid = self.data[index][0]\n",
        "\t\tiid = self.data[index][1]\n",
        "\t\tlabel = self.data[index][2]\n",
        "\t\tu_items = self.u_items_list[uid]\n",
        "\t\tu_users = self.u_users_list[uid]\n",
        "\t\tu_users_items = self.u_users_items_list[uid]\n",
        "\t\ti_users = self.i_users_list[iid]\n",
        "\n",
        "\t\treturn (uid, iid, label), u_items, u_users, u_users_items, i_users\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)"
      ],
      "metadata": {
        "id": "sgKt2lEuw_w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _MultiLayerPercep(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(_MultiLayerPercep, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim // 2, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_dim // 2, output_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "IiMXws5MxGRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _Aggregation(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(_Aggregation, self).__init__()\n",
        "        self.aggre = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim, bias=True),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.aggre(x)"
      ],
      "metadata": {
        "id": "8ezqJNyzxRDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _UserModel(nn.Module):\n",
        "    ''' User modeling to learn user latent factors.\n",
        "    User modeling leverages two types aggregation: item aggregation and social aggregation\n",
        "    '''\n",
        "    def __init__(self, emb_dim, user_emb, item_emb, rate_emb):\n",
        "        super(_UserModel, self).__init__()\n",
        "        self.user_emb = user_emb\n",
        "        self.item_emb = item_emb\n",
        "        self.rate_emb = rate_emb\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.w1 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w2 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w3 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w4 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w5 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w6 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w7 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "\n",
        "        self.g_v = _MultiLayerPercep(2 * self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.user_items_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_items = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.user_items_att_s1 = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_items_s1 = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "        self.user_users_att_s2 = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_neigbors_s2 = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.u_user_users_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.u_aggre_neigbors = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "\n",
        "\n",
        "        self.combine_mlp = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(2 * self.emb_dim, 2*self.emb_dim, bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(2*self.emb_dim, self.emb_dim, bias = True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # used for preventing zero div error when calculating softmax score\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def forward(self, uids, u_item_pad, u_user_pad, u_user_item_pad):\n",
        "        # item aggregation\n",
        "        q_a = self.item_emb(u_item_pad[:,:,0])   # B x maxi_len x emb_dim\n",
        "        mask_u = torch.where(u_item_pad[:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))   # B x maxi_len\n",
        "        u_item_er = self.rate_emb(u_item_pad[:, :, 1])  # B x maxi_len x emb_dim\n",
        "        x_ia = self.g_v(torch.cat([q_a, u_item_er], dim=2).view(-1, 2 * self.emb_dim)).view(q_a.size())  # B x maxi_len x emb_dim\n",
        "        p_i = mask_u.unsqueeze(2).expand_as(q_a) * self.user_emb(uids).unsqueeze(1).expand_as(q_a)  # B x maxi_len x emb_dim\n",
        "\n",
        "        alpha = self.user_items_att(torch.cat([self.w1(x_ia), self.w1(p_i)], dim = 2).view(-1, 2 * self.emb_dim)).view(mask_u.size()) # B x maxi_len\n",
        "        alpha = torch.exp(alpha) * mask_u\n",
        "        alpha = alpha / (torch.sum(alpha, 1).unsqueeze(1).expand_as(alpha) + self.eps)\n",
        "\n",
        "        h_iI = self.aggre_items(torch.sum(alpha.unsqueeze(2).expand_as(x_ia) * x_ia, 1))     # B x emb_dim\n",
        "        h_iI = F.dropout(h_iI, 0.2, training=self.training)\n",
        "\n",
        "\n",
        "\n",
        "        # social aggregation\n",
        "        q_a_s = self.item_emb(u_user_item_pad[:,:,:,0])   # B x maxu_len x maxi_len x emb_dim\n",
        "        mask_s = torch.where(u_user_item_pad[:,:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))  # B x maxu_len x maxi_len\n",
        "        p_i_s = mask_s.unsqueeze(3).expand_as(q_a_s) * self.user_emb(u_user_pad).unsqueeze(2).expand_as(q_a_s)  # B x maxu_len x maxi_len x emb_dim\n",
        "        u_user_item_er = self.rate_emb(u_user_item_pad[:, :, :, 1])  # B x maxu_len x maxi_len x emb_dim\n",
        "        x_ia_s = self.g_v(torch.cat([q_a_s, u_user_item_er], dim=3).view(-1, 2 * self.emb_dim)).view(q_a_s.size())  # B x maxu_len x maxi_len x emb_dim\n",
        "\n",
        "        alpha_s = self.user_items_att_s1(torch.cat([self.w4(x_ia_s), self.w4(p_i_s)], dim = 3).view(-1, 2 * self.emb_dim)).view(mask_s.size())    # B x maxu_len x maxi_len\n",
        "        alpha_s = torch.exp(alpha_s) * mask_s\n",
        "        alpha_s = alpha_s / (torch.sum(alpha_s, 2).unsqueeze(2).expand_as(alpha_s) + self.eps)\n",
        "\n",
        "\n",
        "        h_oI_temp = torch.sum(alpha_s.unsqueeze(3).expand_as(x_ia_s) * x_ia_s, 2)    # B x maxu_len x emb_dim\n",
        "        h_oI = self.aggre_items_s1(h_oI_temp.view(-1, self.emb_dim)).view(h_oI_temp.size())  # B x maxu_len x emb_dim\n",
        "        h_oI = F.dropout(h_oI, p=0.2, training=self.training)\n",
        "\n",
        "        ## calculate attention scores in social aggregation\n",
        "        mask_su = torch.where(u_user_pad > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
        "\n",
        "        beta = self.user_users_att_s2(torch.cat([self.w5(h_oI), self.w5(self.user_emb(u_user_pad))], dim = 2).view(-1, 2 * self.emb_dim)).view(u_user_pad.size())\n",
        "        beta = torch.exp(beta) * mask_su\n",
        "        beta = beta / (torch.sum(beta, 1).unsqueeze(1).expand_as(beta) + self.eps)\n",
        "        h_iS = self.aggre_neigbors_s2(torch.sum(beta.unsqueeze(2).expand_as(h_oI) * h_oI, 1))     # B x emb_dim\n",
        "        h_iS = F.dropout(h_iS, p=0.2, training=self.training)\n",
        "        h =  self.combine_mlp(torch.cat([h_iI, h_iS], dim = 1))\n",
        "\n",
        "        return h"
      ],
      "metadata": {
        "id": "7HCOkOwixUFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _ItemModel(nn.Module):\n",
        "    '''Item modeling to learn item latent factors.\n",
        "    '''\n",
        "    def __init__(self, emb_dim, user_emb, item_emb, rate_emb):\n",
        "        super(_ItemModel, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.user_emb = user_emb\n",
        "        self.item_emb = item_emb\n",
        "        self.rate_emb = rate_emb\n",
        "\n",
        "        self.w1 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w2 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w3 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w4 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.g_u = _MultiLayerPercep(2 * self.emb_dim, self.emb_dim)\n",
        "        self.g_v = _MultiLayerPercep(2 * self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.item_users_att_i = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_users_i = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.item_users_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_users = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.i_friends_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_i_friends = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.if_friends_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_if_friends = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.combine_mlp = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(3* self.emb_dim, 2*self.emb_dim, bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(2*self.emb_dim, self.emb_dim, bias = True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # used for preventing zero div error when calculating softmax score\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def forward(self, iids, i_user_pad):\n",
        "        # user aggregation\n",
        "        p_t = self.user_emb(i_user_pad[:,:,0])\n",
        "        mask_i = torch.where(i_user_pad[:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
        "        i_user_er = self.rate_emb(i_user_pad[:,:,1])\n",
        "        f_jt = self.g_u(torch.cat([p_t, i_user_er], dim = 2).view(-1, 2 * self.emb_dim)).view(p_t.size())\n",
        "\n",
        "        # calculate attention scores in user aggregation\n",
        "        q_j = mask_i.unsqueeze(2).expand_as(f_jt) * self.item_emb(iids).unsqueeze(1).expand_as(f_jt)\n",
        "\n",
        "        miu = self.item_users_att_i(torch.cat([self.w1(f_jt), self.w1(q_j)], dim = 2).view(-1, 2 * self.emb_dim)).view(mask_i.size())\n",
        "        miu = torch.exp(miu) * mask_i\n",
        "        miu = miu / (torch.sum(miu, 1).unsqueeze(1).expand_as(miu) + self.eps)\n",
        "        z_j = self.aggre_users_i(torch.sum(miu.unsqueeze(2).expand_as(f_jt) * self.w1(f_jt), 1))\n",
        "        z_j = F.dropout(z_j, p=0.2, training=self.training)\n",
        "\n",
        "\n",
        "        return z_j"
      ],
      "metadata": {
        "id": "qNGofdW8x-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GraphRec(nn.Module):\n",
        "    '''GraphRec model proposed in the paper Graph neural network for social recommendation\n",
        "\n",
        "    Args:\n",
        "        number_users: the number of users in the dataset.\n",
        "        number_items: the number of items in the dataset.\n",
        "        num_rate_levels: the number of rate levels in the dataset.\n",
        "        emb_dim: the dimension of user and item embedding (default = 64).\n",
        "\n",
        "    '''\n",
        "    def __init__(self, num_users, num_items, num_rate_levels, emb_dim = 64):\n",
        "        super(GraphRec, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.num_rate_levels = num_rate_levels\n",
        "        self.emb_dim = emb_dim\n",
        "        self.user_emb = nn.Embedding(self.num_users, self.emb_dim, padding_idx = 0)\n",
        "        self.item_emb = nn.Embedding(self.num_items, self.emb_dim, padding_idx = 0)\n",
        "        self.rate_emb = nn.Embedding(self.num_rate_levels, self.emb_dim, padding_idx = 0)\n",
        "\n",
        "        self.user_model = _UserModel(self.emb_dim, self.user_emb, self.item_emb, self.rate_emb)\n",
        "\n",
        "        self.item_model = _ItemModel(self.emb_dim, self.user_emb, self.item_emb, self.rate_emb)\n",
        "\n",
        "\n",
        "        self.rate_pred = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(2* self.emb_dim, self.emb_dim, bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(self.emb_dim, self.emb_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(self.emb_dim // 4, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, uids, iids, u_item_pad, u_user_pad, u_user_item_pad, i_user_pad):\n",
        "        '''\n",
        "        Args:\n",
        "            uids: the user id sequences.\n",
        "            iids: the item id sequences.\n",
        "            u_item_pad: the padded user-item graph.\n",
        "            u_user_pad: the padded user-user graph.\n",
        "            u_user_item_pad: the padded user-user-item graph.\n",
        "            i_user_pad: the padded item-user graph.\n",
        "\n",
        "        Shapes:\n",
        "            uids: (B).\n",
        "            iids: (B).\n",
        "            u_item_pad: (B, ItemSeqMaxLen, 2).\n",
        "            u_user_pad: (B, UserSeqMaxLen).\n",
        "            u_user_item_pad: (B, UserSeqMaxLen, ItemSeqMaxLen, 2).\n",
        "            i_user_pad: (B, UserSeqMaxLen, 2).\n",
        "\n",
        "        Returns:\n",
        "            the predicted rate scores of the user to the item.\n",
        "        '''\n",
        "\n",
        "        h = self.user_model(uids, u_item_pad, u_user_pad, u_user_item_pad)\n",
        "        z = self.item_model(iids, i_user_pad)\n",
        "\n",
        "        r_ij = self.rate_pred(torch.cat([h,z], dim = 1))\n",
        "\n",
        "        return r_ij"
      ],
      "metadata": {
        "id": "3XyXbIAyyD8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "truncate_len = 30\n",
        "truncate_len_i = 10\n",
        "\n",
        "\"\"\"\n",
        "Ciao dataset info:\n",
        "Avg number of items rated per user: 38.3\n",
        "Avg number of users interacted per user: 2.7\n",
        "Avg number of users connected per item: 16.4\n",
        "\"\"\"\n",
        "\n",
        "def collate_fn(batch_data):\n",
        "    \"\"\"This function will be used to pad the graph to max length in the batch\n",
        "       It will be used in the Dataloader\n",
        "    \"\"\"\n",
        "    uids, iids, labels = [], [], []\n",
        "    u_items, u_users, u_users_items, i_users = [], [], [], []\n",
        "    u_items_len, u_users_len, i_users_len = [], [], []\n",
        "    count = 0\n",
        "    for data, u_items_u, u_users_u, u_users_items_u, i_users_i in batch_data:\n",
        "        (uid, iid, label) = data\n",
        "        uids.append(uid)\n",
        "        iids.append(iid)\n",
        "        labels.append(label)\n",
        "\n",
        "        # user-items\n",
        "        if len(u_items_u) <= truncate_len:\n",
        "            u_items.append(u_items_u)\n",
        "        else:\n",
        "            u_items.append(random.sample(u_items_u, truncate_len))\n",
        "        u_items_len.append(min(len(u_items_u), truncate_len))\n",
        "\n",
        "\n",
        "        # user-users and user-users-items\n",
        "        if len(u_users_u) < truncate_len:\n",
        "            tmp_users = [item for item in u_users_u]\n",
        "            tmp_users.append(uid)\n",
        "            u_users.append(tmp_users)\n",
        "            u_u_items = []\n",
        "            for uui in u_users_items_u:\n",
        "                if len(uui) < truncate_len:\n",
        "                    u_u_items.append(uui)\n",
        "                else:\n",
        "                    u_u_items.append(random.sample(uui, truncate_len))\n",
        "            # self -loop\n",
        "            u_u_items.append(u_items[-1])\n",
        "            u_users_items.append(u_u_items)\n",
        "\n",
        "        else:\n",
        "            sample_index = random.sample(list(range(len(u_users_u))), truncate_len-1)\n",
        "            tmp_users = [u_users_u[si] for si in sample_index]\n",
        "            tmp_users.append(uid)\n",
        "            u_users.append(tmp_users)\n",
        "\n",
        "            u_users_items_u_tr = [u_users_items_u[si] for si in sample_index]\n",
        "            u_u_items = []\n",
        "            for uui in u_users_items_u_tr:\n",
        "                if len(uui) < truncate_len:\n",
        "                    u_u_items.append(uui)\n",
        "                else:\n",
        "                    u_u_items.append(random.sample(uui, truncate_len))\n",
        "            u_u_items.append(u_items[-1])\n",
        "            u_users_items.append(u_u_items)\n",
        "\n",
        "        u_users_len.append(min(len(u_users_u)+1, truncate_len))\n",
        "\n",
        "        # item-users\n",
        "        if len(i_users_i) <= truncate_len:\n",
        "            i_users.append(i_users_i)\n",
        "        else:\n",
        "            i_users.append(random.sample(i_users_i, truncate_len))\n",
        "        i_users_len.append(min(len(i_users_i), truncate_len))\n",
        "\n",
        "\n",
        "    batch_size = len(batch_data)\n",
        "\n",
        "    # padding\n",
        "    u_items_maxlen = max(u_items_len)\n",
        "    u_users_maxlen = max(u_users_len)\n",
        "    i_users_maxlen = max(i_users_len)\n",
        "\n",
        "\n",
        "    u_item_pad = torch.zeros([batch_size, u_items_maxlen, 2], dtype=torch.long)\n",
        "    for i, ui in enumerate(u_items):\n",
        "        u_item_pad[i, :len(ui), :] = torch.LongTensor(ui)\n",
        "\n",
        "    u_user_pad = torch.zeros([batch_size, u_users_maxlen], dtype=torch.long)\n",
        "    for i, uu in enumerate(u_users):\n",
        "        u_user_pad[i, :len(uu)] = torch.LongTensor(uu)\n",
        "\n",
        "\n",
        "    u_user_item_pad = torch.zeros([batch_size, u_users_maxlen, u_items_maxlen, 2], dtype=torch.long)\n",
        "    for i, uu_items in enumerate(u_users_items):\n",
        "        for j, ui in enumerate(uu_items):\n",
        "            u_user_item_pad[i, j, :len(ui), :] = torch.LongTensor(ui)\n",
        "\n",
        "    i_user_pad = torch.zeros([batch_size, i_users_maxlen, 2], dtype=torch.long)\n",
        "    for i, iu in enumerate(i_users):\n",
        "        i_user_pad[i, :len(iu), :] = torch.LongTensor(iu)\n",
        "\n",
        "\n",
        "    return torch.LongTensor(uids), torch.LongTensor(iids), torch.FloatTensor(labels), \\\n",
        "            u_item_pad, u_user_pad, u_user_item_pad, i_user_pad"
      ],
      "metadata": {
        "id": "IFRPQn9qyLmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.io import loadmat\n",
        "\n",
        "random.seed(1234)\n",
        "\n",
        "workdir = 'datasets/'\n",
        "\n",
        "click_list = []\n",
        "trust_list = []\n",
        "\n",
        "u_items_list = []\n",
        "u_users_list = []\n",
        "u_users_items_list = []\n",
        "i_users_list = []\n",
        "\n",
        "pos_u_items_list = []\n",
        "pos_i_users_list = []\n",
        "\n",
        "user_count = 0\n",
        "item_count = 0\n",
        "rate_count = 0\n",
        "\n",
        "for s in click_f:\n",
        "\tuid = s[0]\n",
        "\tiid = s[1]\n",
        "\tlabel = s[2]\n",
        "\n",
        "\tif uid > user_count:\n",
        "\t\tuser_count = uid\n",
        "\tif iid > item_count:\n",
        "\t\titem_count = iid\n",
        "\tif label > rate_count:\n",
        "\t\trate_count = label\n",
        "\tclick_list.append([uid, iid, label])\n",
        "\n",
        "pos_list = []\n",
        "for i in range(len(click_list)):\n",
        "\tpos_list.append((click_list[i][0], click_list[i][1], click_list[i][2]))\n",
        "pos_list = list(set(pos_list))\n",
        "\n",
        "\n",
        "pos_df = pd.DataFrame(pos_list, columns = ['uid', 'iid', 'label'])\n",
        "filter_pos_list = []\n",
        "user_in_set, user_out_set = set(), set()\n",
        "for u in tqdm(range(user_count + 1)):\n",
        "\thist = pos_df[pos_df['uid'] == u]\n",
        "\tif len(hist) < 5:\n",
        "\t\tuser_out_set.add(u)\n",
        "\t\tcontinue\n",
        "\tuser_in_set.add(u)\n",
        "\tu_items = hist['iid'].tolist()\n",
        "\tu_ratings = hist['label'].tolist()\n",
        "\tfilter_pos_list.extend([(u, iid, rating) for iid, rating in zip(u_items, u_ratings)])\n",
        "print('user in and out size: ', len(user_in_set), len(user_out_set))\n",
        "print('data size before and after filtering: ', len(pos_list), len(filter_pos_list))\n",
        "\n",
        "# train, valid and test data split\n",
        "pos_list = filter_pos_list\n",
        "\n",
        "random.shuffle(pos_list)\n",
        "num_test = int(len(pos_list) * 0.2)\n",
        "test_set = pos_list[:num_test]\n",
        "valid_set = pos_list[num_test:2 * num_test]\n",
        "train_set = pos_list[2 * num_test:]\n",
        "\n",
        "\n",
        "print('Train samples: {}, Valid samples: {}, Test samples: {}, Total samples: {}'.format(len(train_set), len(valid_set), len(test_set), len(pos_list)))\n",
        "\n",
        "\n",
        "pos_df = pd.DataFrame(pos_list, columns = ['uid', 'iid', 'label'])\n",
        "train_df = pd.DataFrame(train_set, columns = ['uid', 'iid', 'label'])\n",
        "valid_df = pd.DataFrame(valid_set, columns = ['uid', 'iid', 'label'])\n",
        "test_df = pd.DataFrame(test_set, columns = ['uid', 'iid', 'label'])\n",
        "\n",
        "click_df = pd.DataFrame(click_list, columns = ['uid', 'iid', 'label'])\n",
        "train_df = train_df.sort_values(axis = 0, ascending = True, by = 'uid')\n",
        "pos_df = pos_df.sort_values(axis = 0, ascending = True, by = 'uid')\n",
        "\n",
        "\n",
        "for u in tqdm(range(user_count + 1)):\n",
        "\thist = train_df[train_df['uid'] == u]\n",
        "\tu_items = hist['iid'].tolist()\n",
        "\tu_ratings = hist['label'].tolist()\n",
        "\tif u_items == []:\n",
        "\t\tu_items_list.append([(0, 0)])\n",
        "\telse:\n",
        "\t\tu_items_list.append([(iid, rating) for iid, rating in zip(u_items, u_ratings)])\n",
        "\n",
        "\n",
        "\n",
        "train_df = train_df.sort_values(axis = 0, ascending = True, by = 'iid')\n",
        "\n",
        "\n",
        "userful_item_set = set()\n",
        "for i in tqdm(range(item_count + 1)):\n",
        "\thist = train_df[train_df['iid'] == i]\n",
        "\ti_users = hist['uid'].tolist()\n",
        "\ti_ratings = hist['label'].tolist()\n",
        "\tif i_users == []:\n",
        "\t\ti_users_list.append([(0, 0)])\n",
        "\telse:\n",
        "\t\ti_users_list.append([(uid, rating) for uid, rating in zip(i_users, i_ratings)])\n",
        "\t\tuserful_item_set.add(i)\n",
        "\n",
        "print('item size before and after filtering: ', item_count, len(userful_item_set))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count_f_origin, count_f_filter = 0,0\n",
        "for s in trust_f:\n",
        "\tuid = s[0]\n",
        "\tfid = s[1]\n",
        "\tcount_f_origin += 1\n",
        "\tif uid > user_count or fid > user_count:\n",
        "\t\tcontinue\n",
        "\tif uid in user_out_set or fid in user_out_set:\n",
        "\t\tcontinue\n",
        "\ttrust_list.append([uid, fid])\n",
        "\tcount_f_filter += 1\n",
        "\n",
        "print('u-u relation filter size changes: ', count_f_origin, count_f_filter)\n",
        "trust_df = pd.DataFrame(trust_list, columns = ['uid', 'fid'])\n",
        "trust_df = trust_df.sort_values(axis = 0, ascending = True, by = 'uid')\n",
        "\n",
        "\n",
        "\n",
        "count_0, count_1 = 0,0\n",
        "for u in tqdm(range(user_count + 1)):\n",
        "\thist = trust_df[trust_df['uid'] == u]\n",
        "\tu_users = hist['fid'].unique().tolist()\n",
        "\tif u_users == []:\n",
        "\t\tu_users_list.append([0])\n",
        "\t\tu_users_items_list.append([[(0,0)]])\n",
        "\t\tcount_0 += 1\n",
        "\telse:\n",
        "\t\tu_users_list.append(u_users)\n",
        "\t\tuu_items = []\n",
        "\t\tfor uid in u_users:\n",
        "\t\t\tuu_items.append(u_items_list[uid])\n",
        "\t\tu_users_items_list.append(uu_items)\n",
        "\t\tcount_1 += 1\n",
        "print('trust user with items size: ', count_0, count_1)\n",
        "\n"
      ],
      "metadata": {
        "id": "CWwgSipoySkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import argparse\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn"
      ],
      "metadata": {
        "id": "W3JtiANPyXE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_data = GRDataset(train_set, u_items_list,  u_users_list, u_users_items_list, i_users_list)\n",
        "valid_data = GRDataset(valid_set, u_items_list,  u_users_list, u_users_items_list, i_users_list)\n",
        "test_data = GRDataset(test_set, u_items_list, u_users_list,  u_users_items_list, i_users_list)\n",
        "train_loader = DataLoader(train_data, batch_size = 128, shuffle = True, collate_fn = collate_fn)\n",
        "valid_loader = DataLoader(valid_data, batch_size = 128, shuffle = False, collate_fn = collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size = 128, shuffle = False, collate_fn = collate_fn)\n",
        "\n",
        "model = GraphRec(user_count+1, item_count+1, rate_count+1, 64).to(device)"
      ],
      "metadata": {
        "id": "bJCIHVJayZno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "_-6DzQw9yb_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainForEpoch(train_list,train_loader, model, optimizer, epoch, num_epochs, criterion, log_aggr=1):\n",
        "    model.train()\n",
        "\n",
        "    sum_epoch_loss = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for i, (uids, iids, labels, u_items, u_users, u_users_items, i_users) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        uids = uids.to(device)\n",
        "        iids = iids.to(device)\n",
        "        labels = labels.to(device)\n",
        "        u_items = u_items.to(device)\n",
        "        u_users = u_users.to(device)\n",
        "        u_users_items = u_users_items.to(device)\n",
        "        i_users = i_users.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(uids, iids, u_items, u_users, u_users_items, i_users)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_val = loss.item()\n",
        "        sum_epoch_loss += loss_val\n",
        "\n",
        "        iter_num = epoch * len(train_loader) + i + 1\n",
        "        train_list[0].append(loss_val)\n",
        "        train_list[1].append(sum_epoch_loss / (i + 1))\n",
        "        if i % log_aggr == 0:\n",
        "            print('[TRAIN WWW] epoch %d/%d batch loss: %.4f (avg %.4f) (%.2f im/s)'\n",
        "                % (epoch + 1, num_epochs, loss_val, sum_epoch_loss / (i + 1),\n",
        "                  len(uids) / (time.time() - start)))\n",
        "        start = time.time()\n",
        "    return train_list"
      ],
      "metadata": {
        "id": "MyhD7wI3ydv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valid_loader, model):\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for uids, iids, labels, u_items, u_users, u_users_items, i_users in tqdm(valid_loader):\n",
        "            uids = uids.to(device)\n",
        "            iids = iids.to(device)\n",
        "            labels = labels.to(device)\n",
        "            u_items = u_items.to(device)\n",
        "            u_users = u_users.to(device)\n",
        "            u_users_items = u_users_items.to(device)\n",
        "            i_users = i_users.to(device)\n",
        "\n",
        "            preds = model(uids, iids, u_items, u_users, u_users_items, i_users)\n",
        "\n",
        "            error = torch.abs(preds.squeeze(1) - labels)\n",
        "            errors.extend(error.data.cpu().numpy().tolist())\n",
        "\n",
        "    mae = np.mean(errors)\n",
        "    rmse = np.sqrt(np.mean(np.power(errors, 2)))\n",
        "    return mae, rmse"
      ],
      "metadata": {
        "id": "MZFBRr5eygwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "scheduler = StepLR(optimizer, step_size = 30, gamma = 0.5)\n",
        "train_list = [[],[]]\n",
        "valid_loss_list, test_loss_list = [],[]\n",
        "fn = 'graphrec'\n",
        "for epoch in tqdm(range(30)):\n",
        "    # train for one epoch\n",
        "    scheduler.step(epoch = epoch)\n",
        "    trainForEpoch(train_list,train_loader, model, optimizer, epoch, 30, criterion, log_aggr = 10)\n",
        "\n",
        "    mae, rmse = validate(valid_loader, model)\n",
        "    valid_loss_list.append([mae, rmse])\n",
        "\n",
        "    test_mae, test_rmse = validate(test_loader, model)\n",
        "    test_loss_list.append([test_mae, test_rmse])\n",
        "\n",
        "    # store best loss and save a model checkpoint\n",
        "    ckpt_dict = {\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "\n",
        "    #torch.save(ckpt_dict, '%s/random_latest_checkpoint.pth.tar' %fn)\n",
        "\n",
        "    if epoch == 0:\n",
        "        best_mae = mae\n",
        "    elif mae < best_mae:\n",
        "        best_mae = mae\n",
        "        print(ckpt_dict)\n",
        "\n",
        "    print('Epoch {} validation: MAE: {:.4f}, RMSE: {:.4f}, Best MAE: {:.4f}, test_MAE: {:.4f}, test_RMSE: {:.4f}'.format(epoch, mae, rmse, best_mae, test_mae, test_rmse))"
      ],
      "metadata": {
        "id": "t6-AX2tRyjH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "iters = [iter  for iter in range(len(m))]\n",
        "plt.plot(iters, m, label='validation')\n",
        "plt.plot(iters, n, label='test')\n",
        "# plt.plot(iters, test_losslist, label='test')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.title('test and validation loss curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GzeYa5XiymUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}